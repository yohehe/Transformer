{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import math\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from torchvision import transforms\n",
    "from torch.nn import Module\n",
    "from torchvision import models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. nn.TransformerEncoderの使い方\n",
    "\n",
    "[reference]\n",
    "https://discuss.pytorch.org/t/how-to-add-padding-mask-to-nn-transformerencoder-module/63390/2\n",
    "\n",
    "\n",
    "### The required shapes are shown in nn.Transformer.forward\n",
    "\n",
    "[入力 shape]:\n",
    "1. Eembeddingされたtensor (Sequence length, Batch_Size, Embedding dim)\n",
    "2. src_mask: (Sequence_lehgh,Seqnence_length)\n",
    "3. src_key_padding_mask: (Batch_size,Sequence_length)\n",
    "\n",
    "#### batch_firstではないことに注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 基本：maskなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 10]) torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "# shape:(Sequence length, Batch_Size, Embedding dim)\n",
    "q = torch.randn(3,1,10)\n",
    "\n",
    "# embedding size: 10, multi attention head :one head\n",
    "attn = nn.MultiheadAttention(10, 1) \n",
    "\n",
    "#maskなしでの出力の場合は以下で\n",
    "output,atten_map=attn(q,q,q)\n",
    "print(output.shape,atten_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. maskを設定する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#三角行列を用いたmask\n",
    "def src_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000],\n",
       "         [0.4852, 0.5148, 0.0000],\n",
       "         [0.3130, 0.3614, 0.3256]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attention_mapを設定\n",
    "#sequence_len:3のため引数は3\n",
    "#src_mask: (Sequence_lehgh,Seqnence_length)\n",
    "\n",
    "# attention output weights\n",
    "attn(q, q, q, attn_mask=src_mask(3))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.padding_indexを指定してmask指定したい場合\n",
    " \n",
    "\n",
    "src_key_padding_maskのshape: (Batch_size,Sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "#padding_indexの部分をTrueとしてboolを作成\n",
    "#sequence_length:3で index3を学習させたくない場合以下の様に指定\n",
    "src_key_padding_mask = torch.tensor([[0, 0, 1]]).bool()\n",
    "print(src_key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000],\n",
       "         [0.4852, 0.5148, 0.0000],\n",
       "         [0.4641, 0.5359, 0.0000]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, q, q, attn_mask=src_mask(3), key_padding_mask=src_key_padding_mask)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 5])\n",
      "tensor([[[ 0.7567,  1.1566, -0.9552,  0.2868, -0.2045],\n",
      "         [ 0.4830,  0.0679, -0.7548, -1.0602, -0.5305],\n",
      "         [-1.6067,  0.1129, -1.3658,  0.1027,  0.0240],\n",
      "         [ 1.1009, -0.4258, -0.1315, -0.9095,  1.4193],\n",
      "         [ 0.4125,  1.4098,  0.1352,  0.9740, -0.4609],\n",
      "         [-1.6067,  0.1129, -1.3658,  0.1027,  0.0240],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "#sampleのsetenceデータを作成\n",
    "sample_sentence=torch.LongTensor([1,2,3,5,4,3,0,0,0,0]).unsqueeze(0)\n",
    "\n",
    "#embedding paddingを0で指定\n",
    "embedding=nn.Embedding(6,5,padding_idx=0)\n",
    "\n",
    "#embedding\n",
    "embed=embedding(sample_sentence)\n",
    "print(embed.shape)\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####　key_padding_maskはbool値で指定可\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 5, 4, 3, 0, 0, 0, 0]])\n",
      "tensor([[False, False, False, False, False, False,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "# padding_indexを学習させないようにするにはkeypaddingを設定する必要がある。\n",
    "# 上記データの場合 sample_sentenceに格納されているデータは、0,1,2,3,4,5\n",
    "print(sample_sentence)\n",
    "padding_mask= (sample_sentence==0).bool()\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 5])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=embed.transpose(0,1)\n",
    "inputs.shape\n",
    "#sequence_length,batch_size,embedding_dimsの順番に変更する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#三角行列を用いたmask\n",
    "def src_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.5885, 0.4115, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.4047, 0.4630, 0.1323, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.2140, 0.2022, 0.2418, 0.3421, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.1686, 0.1086, 0.2827, 0.0654, 0.3747, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.1835, 0.2099, 0.0600, 0.3218, 0.1648, 0.0600, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = nn.MultiheadAttention(5, 1) # embedding size 10, one head\n",
    "\n",
    "attn(inputs,inputs,inputs, \n",
    "     attn_mask=src_mask(10),\n",
    "     key_padding_mask=padding_mask)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記をまとめて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mha_output(input_sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "    inputs_sentenceにはstoiで変換されたtorch.LongTensorを入力\n",
    "    \"\"\"\n",
    "    \n",
    "    #embedding:\n",
    "    embed=embedding(input_sentence)\n",
    "    \n",
    "    #mask: paddingの項目をTrueのboolを\n",
    "    mask_pad=0\n",
    "    key_padding_mask=(input_sentence==mask_pad).bool()\n",
    "    \n",
    "    #入力がbatch_firstではないため注意\n",
    "    #sequence_length,batch_size,embedding_dimsの順番に変更する。\n",
    "    embed=embed.transpose(0,1)\n",
    "    seq_len=embed.shape[0]\n",
    "\n",
    "    #attention output\n",
    "    output,atten_mask=attn(embed,embed,embed,\n",
    "                            attn_mask=src_mask(seq_len),\n",
    "                            key_padding_mask=key_padding_mask)\n",
    "    \n",
    "    return output,atten_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5])\n",
      "torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "output,atten_mask=mha_output(sample_sentence)\n",
    "print(output.shape)\n",
    "print(atten_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. nn.TransformerEncoder()の使い方\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 基本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "#基本的な使い方\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 maskを設定したい場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 5])\n",
      "tensor([[[ 0.2856, -0.5371, -1.1387,  0.8060, -0.5655],\n",
      "         [ 0.2140, -0.9235,  1.7130, -0.1160, -0.8278],\n",
      "         [ 0.6873, -0.5021, -0.0607,  1.9373, -0.7169],\n",
      "         [ 1.0140, -0.9519, -0.5368, -0.7806, -0.7649],\n",
      "         [ 1.2167,  0.8828, -0.4382, -0.8354, -0.9914],\n",
      "         [ 0.6873, -0.5021, -0.0607,  1.9373, -0.7169],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "#sampleのsetenceデータを作成\n",
    "sample_sentence=torch.LongTensor([1,2,3,5,4,3,0,0,0,0]).unsqueeze(0)\n",
    "\n",
    "#embedding paddingを0で指定\n",
    "embedding=nn.Embedding(6,5,padding_idx=0)\n",
    "\n",
    "embed=embedding(sample_sentence)\n",
    "print(embed.shape)\n",
    "print(embed)\n",
    "#paddingのembeddingの値を0に設定しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maskの設定は同様\n",
    "def src_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_modelはembedding dimの次元数を設定\n",
    "#d_model/nheadは割り切れる数で設定しないとerror(ex. d_model=5/nhead=2)だとerror\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=5, nhead=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "inputs=embed.transpose(0,1)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False, False,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "#paddingのstoiを指定\n",
    "mask_pad=0\n",
    "#bool値でpadding_maskを作成\n",
    "key_padding_mask=(sample_sentence==0).bool()\n",
    "print(key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "out=transformer_encoder(inputs,\n",
    "                        mask=src_mask(10),\n",
    "                        src_key_padding_mask=key_padding_mask)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transformerを用いたEncoder and Decoder\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "\n",
    "#### なぜ math.sqrt（）でかけてあるのか。\n",
    "\n",
    "回答1\\\n",
    "https://stackoverflow.com/questions/56930821/why-does-embedding-vector-multiplied-by-a-constant-in-transformer-model\n",
    "\n",
    "\n",
    "回答2\\\n",
    "https://datascience.stackexchange.com/questions/87906/transformer-model-why-are-word-embeddings-scaled-before-adding-positional-encod\n",
    "\n",
    "The reason we increase the embedding values before the addition is to make the positional encoding relatively smaller. This means the original meaning in the embedding vector won’t be lost when we add them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 maskなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_dim,num_token):\n",
    "        super().__init__()\n",
    "        \n",
    "        #embedding:\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_dim)\n",
    "        self.embed_dim=embed_dim\n",
    "        \n",
    "        #encoder\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model=embed_dim,\n",
    "                                                        nhead=1)\n",
    "\n",
    "        self.transformer_encoder=nn.TransformerEncoder(encoder_layer,\n",
    "                                                         num_layers=6)\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder=nn.Linear(embed_dim,num_token)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #positional Encoderを使用していない場合も*math.sqrt()が必要か不明\n",
    "        out=self.embedding(x)*math.sqrt(self.embed_dim)\n",
    "        out=out.transpose(0,1)\n",
    "        \n",
    "        out=self.transformer_encoder(out)\n",
    "        \n",
    "        out=self.decoder(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 41])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence=torch.LongTensor([1,2,3,5,4,3,0,0,0,0]).unsqueeze(0)\n",
    "\n",
    "t=Transformer(vocab_size=10,embed_dim=5,num_token=41)\n",
    "out=t(sample_sentence)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 maskあり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_dim,num_token):\n",
    "        super().__init__()\n",
    "        \n",
    "        #embedding:\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_dim)\n",
    "        self.embed_dim=embed_dim\n",
    "        self.vocab_size=vocab_size\n",
    "        \n",
    "        #encoder\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model=embed_dim,\n",
    "                                                        nhead=1)\n",
    "        \n",
    "        self.transformer_encoder=nn.TransformerEncoder(encoder_layer,\n",
    "                                                         num_layers=6)\n",
    "\n",
    "        #decoder\n",
    "        self.decoder=nn.Linear(embed_dim,num_token)\n",
    "        \n",
    "    #mask時に使用する。maskの設定は同様\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        #positional Encoderを使用していない場合も*math.sqrt()が必要か不明\n",
    "        out=self.embedding(x)*math.sqrt(self.embed_dim)\n",
    "        \n",
    "        out=out.transpose(0,1)\n",
    "        \n",
    "        if mask is not None:\n",
    "            \n",
    "            key_padding_mask = (x == 0).bool()\n",
    "            out=self.transformer_encoder(out,\n",
    "                                        mask=mask,\n",
    "                                        src_key_padding_mask=key_padding_mask\n",
    "                                        )\n",
    "        #decoder\n",
    "        out=self.decoder(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Transformer_(vocab_size=10,embed_dim=5,num_token=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "#sampleのstoiされた文章データ\n",
    "sample_sentence=torch.LongTensor([1,2,3,5,4,3,0,0,0,0]).unsqueeze(0)\n",
    "print(sample_sentence.shape)\n",
    "\n",
    "#maskを作成\n",
    "src_mask=model.generate_square_subsequent_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "out=model(sample_sentence,src_mask)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 loss出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshape Data\n",
    "out=out.view(-1,10)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2568, grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss=criterion(out.unsqueeze(0),sample_sentence)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
